{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import argparse\n",
    "import sys\n",
    "import shap\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def main(args, sampler, scorer, conv_expl=None):\n",
    "    if args['randomize_data']:\n",
    "        sampler.randomize()\n",
    "    conv_to_explain = []\n",
    "    background_data = []\n",
    "    explain_flag = True\n",
    "    background_flag = True\n",
    "    conv_it = sampler.get_next()\n",
    "    random_conv_it = sampler.get_next_random()\n",
    "    num_iteras = max(args['num_examples'], args['background_examples']) if args[\"all\"] == False else len(sampler)\n",
    "    print('Number of conversations to analyze:', args['num_examples'] if args[\"all\"] == False else len(sampler))\n",
    "    for i in range(num_iteras):\n",
    "        # print(i)\n",
    "        # if i == len(sampler) - 1:\n",
    "        #     break\n",
    "        try:\n",
    "            econv = next(conv_it).strip()\n",
    "        except StopIteration as _:\n",
    "            explain_flag = False\n",
    "        try:\n",
    "            bconv = next(random_conv_it).strip()\n",
    "        except StopIteration as _:\n",
    "            background_flag = False\n",
    "        if explain_flag:\n",
    "            conv_to_explain.append(econv)\n",
    "        if background_flag:\n",
    "            background_data.append(bconv)\n",
    "        if not args['all'] and (i + 1) == args['num_examples']:\n",
    "            explain_flag = False\n",
    "        if (i + 1) == args['background_examples']:\n",
    "            background_flag = False\n",
    "    if conv_expl:\n",
    "        conv_to_explain = conv_expl\n",
    "    print(f'Number of conversation: {len(conv_to_explain)}')\n",
    "    print(f\"Number of background conversations: {len(background_data)}\")\n",
    "    print(\"First conversation to explain:\", conv_to_explain[0])\n",
    "    ctxt_responses = []\n",
    "    if args['contains_response']:\n",
    "        fmtd_input = np.array(scorer.format_conversations(conv_to_explain))\n",
    "        bgd_fmtd = np.array(scorer.format_conversations(background_data))\n",
    "    else:\n",
    "        ctxt_to_explain, res_to_explain = zip(*[(conv[:conv.rfind('\\n')], conv.split('\\n')[-1].strip()) for conv in conv_to_explain])\n",
    "        ctxt_bgd, res_bgd = zip(*[(conv[:conv.rfind('\\n')], conv.split('\\n')[-1].strip()) for conv in background_data])\n",
    "        fmtd_input = np.array(scorer.format_conversations(res_to_explain))\n",
    "        bgd_fmtd = np.array(scorer.format_conversations(res_bgd))\n",
    "    \n",
    "    shap_values = []\n",
    "    if args['contains_response']:\n",
    "        response_lengths = [len(scorer.get_tokenizer().encode(conv.split('\\n')[-1])) for conv in bgd_fmtd]\n",
    "        scorer.set_response_lengths(response_lengths)\n",
    "    else:\n",
    "        scorer.set_contexts(ctxt_bgd)\n",
    "    # print(f\"These are the contexts: {ctxt_to_explain}\")\n",
    "    # print(f\"First Background conversation: {background_data[0]}\")\n",
    "    # print(f\"First five background contexts: {ctxt_bgd[:5]}\")\n",
    "    # print(f\"First five background responses: {bgd_fmtd[:5]}\")\n",
    "    if args['algo'] == 'kernel':\n",
    "        explainer = shap.KernelExplainer(scorer.get_scores, (bgd_fmtd, scorer.get_tokenizer()), special_token=50256)\n",
    "    elif args['algo'] == 'partition':\n",
    "        explainer = shap.Explainer(scorer.get_scores, scorer.get_masker())\n",
    "\n",
    "    for i, conv in enumerate(fmtd_input):\n",
    "        if args['contains_response']:\n",
    "            scorer.set_response_lengths([len(scorer._tokenizer.encode(conv.split('\\n')[-1]))])\n",
    "        else:\n",
    "            scorer.set_contexts([ctxt_to_explain[i]])\n",
    "        if args['algo'] == 'kernel':\n",
    "            shap_values.append(explainer.shap_values(np.array([conv]), nsamples=args['n_samples']))\n",
    "        elif args['algo'] == 'partition':\n",
    "            shap_values.append(explainer(np.array([conv])))\n",
    "    return shap_values, fmtd_input"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "args = {'sampler_path' : '/home/ubuntu/adversary_generator/test_conversations/diversity', \\\n",
    "        # 'scorer_path' : '/home/ubuntu/dialogue_evaluation/', \\\n",
    "        'scorer_path' : '/home/ubuntu/DialogRPT/', \\\n",
    "        'randomize_data': False, \\\n",
    "        'all' : True, \\\n",
    "        'num_examples' : 10, \\\n",
    "        'background_examples': 100, \\\n",
    "        'e' : 'vocab', \\\n",
    "        'top_k' : 1, \\\n",
    "        'aggregate' : False, \\\n",
    "        'n_samples' : 1000, \\\n",
    "        'algo' : 'kernel', \\\n",
    "        'contains_response' : False,\n",
    "        'sample_adversary' : True,\n",
    "        'load_saved' : True,\n",
    "        'metric_name' : \"DialogRPT\",\n",
    "        'ds_name' : \"daily_dialogue\",\n",
    "        'save_pickle' : True} #whether the input to the scorer is just the response (False) or response+context (True)\n",
    "output_dir = os.path.join(\"./results\", args[\"metric_name\"], args[\"ds_name\"])\n",
    "print(f\"This is the output directory:  {output_dir}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is the output directory:  ./results/DialogRPT/daily_dialogue\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sampler_path': '/home/ubuntu/adversary_generator/test_conversations/entailment',\n",
       " 'scorer_path': '/home/ubuntu/DialogRPT/',\n",
       " 'randomize_data': False,\n",
       " 'all': True,\n",
       " 'num_examples': 10,\n",
       " 'background_examples': 100,\n",
       " 'e': 'vocab',\n",
       " 'top_k': 1,\n",
       " 'aggregate': False,\n",
       " 'n_samples': 1000,\n",
       " 'algo': 'kernel',\n",
       " 'contains_response': False,\n",
       " 'sample_adversary': True,\n",
       " 'load_saved': True,\n",
       " 'metric_name': 'DialogRPT',\n",
       " 'ds_name': 'daily_dialogue',\n",
       " 'save_pickle': True}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import importlib.util\n",
    "score_spec = importlib.util.spec_from_file_location(\"score\", os.path.join(args[\"scorer_path\"], \"score.py\"))\n",
    "score = importlib.util.module_from_spec(score_spec)\n",
    "score_spec.loader.exec_module(score)\n",
    "if args[\"sample_adversary\"]:\n",
    "    from sample_adversarial import adversary_sampler\n",
    "    sampler = adversary_sampler(args[\"sampler_path\"])\n",
    "else:\n",
    "    sample_spec = importlib.util.spec_from_file_location(\"sample\", os.path.join(args[\"sampler_path\"], \"sample.py\"))\n",
    "    sample = importlib.util.module_from_spec(sample_spec)\n",
    "    sample_spec.loader.exec_module(sample)\n",
    "    sampler = sample.conversation_sampler()\n",
    "# metric_name='context', mask_token='<|endoftext|>', ngrams=2 | these can be the input arguments to the daily dialogue evaluation scorer.\n",
    "scorer = score.conversation_scorer(metric_name='overall', mask_token='<|endoftext|>', ngrams=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Model: updown\n",
      "Loading Model: width\n",
      "Loading Model: depth\n",
      "Loading Model: human-vs-rand\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# scorer(['my name is baber'])\n",
    "# for ix in range(len(sampler)//2):\n",
    "ix = 0\n",
    "print(sampler.candidate_conversations[(ix * 2)])\n",
    "print(\"--------------\")\n",
    "print(sampler.candidate_conversations[(ix * 2) + 1])\n",
    "print(\"--------------\")\n",
    "print(f\"The score difference: {sampler.score_diffs[ix]}\")\n",
    "print(f\"The original scores: {sampler.og_scores[ix]}\")\n",
    "print(\"Average metric scores for human conversations:\", np.average([x[0] for x in sampler.og_scores]))\n",
    "print(\"Average metric scores for adversarial conversations:\", np.average([x[1] for x in sampler.og_scores]))\n",
    "len(sampler)\n",
    "# it = sampler.get_next()\n",
    "# next(it)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Okay . This trail looks the best . It's a little steep . But I'm sure it will be alright .\n",
      "Well . You're the tour guide , I'll follow you .\n",
      "... What a stink . This place stinks like rotten eggs .\n",
      "--------------\n",
      "Okay . This trail looks the best . It's a little steep . But I'm sure it will be alright .\n",
      "Well . You're the tour guide , I'll follow you .\n",
      "Suddenly the ' great outdoors ' isn't so appealing . Let's hike a little faster ...\n",
      "--------------\n",
      "The score difference: 0.5657510216291812\n",
      "The original scores: (0.02466196716220529, 0.5904129887913865)\n",
      "Average metric scores for human conversations: 0.16255878959204428\n",
      "Average metric scores for adversarial conversations: 0.28474506882822487\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# f'{}_shap_values.pkl'\n",
    "output_fname = f'{args[\"sampler_path\"].split(\"/\")[-1]}_shap_values.pkl'\n",
    "output_fname"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'entailment_shap_values.pkl'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# shap_values, conversations = main(args, sampler, scorer, conv_expl=[*sampler.candidate_conversations[:2]])\n",
    "shap_values, conversations = main(args, sampler, scorer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of conversations to analyze: 60\n",
      "Number of conversation: 60\n",
      "Number of background conversations: 30\n",
      "First conversation to explain: Okay . This trail looks the best . It's a little steep . But I'm sure it will be alright .\n",
      "Well . You're the tour guide , I'll follow you .\n",
      "... What a stink . This place stinks like rotten eggs .\n",
      "Data Shape: (30,)\n",
      "Provided model function fails when applied to the provided data set.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-63cc5be7ecf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# shap_values, conversations = main(args, sampler, scorer, conv_expl=[*sampler.candidate_conversations[:2]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-784a30ee59dd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, sampler, scorer, conv_expl)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# print(f\"First five background responses: {bgd_fmtd[:5]}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKernelExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbgd_fmtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'partition'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_masker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shap/shap/explainers/_kernel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, link, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDocumentData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mmodel_null\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_model_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mmodel_null\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_model_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shap/shap/utils/_legacy.py\u001b[0m in \u001b[0;36mmatch_model_to_data\u001b[0;34m(model, data, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mout_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mout_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Provided model function fails when applied to the provided data set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DialogRPT/score.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, to_score)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_cards\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mfinal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DialogRPT/score.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, conversation, card)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_device\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         )\n\u001b[1;32m   1279\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                 )\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship-env/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(len(shap_values))\n",
    "if args[\"save_pickle\"]:\n",
    "    with open(os.path.join(output_dir, output_fname), \"wb\") as shap_pkl_file:\n",
    "        pickle.dump(shap_values, shap_pkl_file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "if args[\"load_saved\"]:\n",
    "    import pickle\n",
    "    with open(\"./results//DialogRPT/daily_dialogue/repetitiveness_shap_values.pkl\", \"rb\") as pshap_vals:\n",
    "        shap_values = pickle.load(pshap_vals)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# for i in range(len(shap_values)):\n",
    "#     print(shap_values[i]0\n",
    "#     break\n",
    "len(shap_values)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_vocab_importance(args, importance_dict, shap_values, conversation=None):\n",
    "    # data_points\n",
    "    if args['algo'] == 'kernel':\n",
    "        shap_vals = shap_values.reshape((-1,))\n",
    "        for i, data_point in enumerate(scorer.get_tokenizer().tokenize(conversation)):\n",
    "            if data_point.startswith('Ġ'):\n",
    "                data_point = data_point[1:]\n",
    "            data_point = data_point.lower()\n",
    "            importance_dict.setdefault(data_point, (0, 0))\n",
    "            importance_dict[data_point] = \\\n",
    "                (importance_dict[data_point][0] + shap_vals[i], importance_dict[data_point][1] + 1)\n",
    "    else:\n",
    "        for i, data_point in enumerate(shap_values.data[0]):\n",
    "            if data_point.startswith('Ġ'):\n",
    "                data_point = data_point[1:].lower()\n",
    "            importance_dict.setdefault(data_point, (0, 0))\n",
    "            importance_dict[data_point] = (importance_dict[data_point][0] + shap_values.values[0][i], importance_dict[data_point][1] + 1)\n",
    "\n",
    "    return importance_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalize_importance(vimportance):\n",
    "    importance = []\n",
    "    words = []\n",
    "    for word in vimportance:\n",
    "        importance.append(vimportance[word])\n",
    "        words.append(word)\n",
    "\n",
    "    importance = np.array(importance)\n",
    "    norm_imp = (importance - min(importance))/(max(importance) - min(importance))\n",
    "    to_return = dict()\n",
    "    for i, word in enumerate(words):\n",
    "        to_return[word] = norm_imp[i]\n",
    "    return to_return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#aggregate importance over all the conversations\n",
    "vimp_human = dict()\n",
    "vimp_adv = dict()\n",
    "for ix in range(len(shap_values)):\n",
    "    response = sampler.candidate_conversations[ix].split('\\n')[-1]\n",
    "    # print(response)\n",
    "    if ix % 2 == 0:\n",
    "        vimp_human = get_vocab_importance(args, vimp_human, shap_values[ix], response)\n",
    "    else:\n",
    "        vimp_adv = get_vocab_importance(args, vimp_adv, shap_values[ix], response)\n",
    "print(vimp_human['i'])\n",
    "vimp_human = {word: vimp_human[word][0]/vimp_human[word][1] for word in vimp_human}\n",
    "vimp_adv = {word: vimp_adv[word][0]/vimp_adv[word][1] for word in vimp_adv}\n",
    "# print(\"Human vocab importance\")\n",
    "vimp_human_norm = normalize_importance(vimp_human)\n",
    "vimp_adv_norm = normalize_importance(vimp_adv)\n",
    "# print(\"Adversarial vocab importance\")\n",
    "# print(normalize_importance(vimp_adv))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# vimp_human_norm['i']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def unroll_path(path):\n",
    "    if len(path) == 0:\n",
    "        return []\n",
    "    return unroll_path(path[0]) + [path[1]] \n",
    "\n",
    "def get_feature_diff(vec1, vec2):\n",
    "    matrix = [[0 for i in range(len(vec1))] for i in range(len(vec2))]\n",
    "    path_matrix = [[tuple() for i in range(len(vec1))] for i in range(len(vec2))]\n",
    "    overall_max = 0\n",
    "    max_ix = (-1, -1)\n",
    "    for i in range(len(vec2)):\n",
    "        for j in range(len(vec1)):\n",
    "            matched = int(vec2[i] == vec1[j])\n",
    "            neighbors = [(0 if (i - 1 < 0) or (j - 1 < 0) else matrix[i-1][j-1]) + matched, 0 if (i - 1 < 0) else matrix[i-1][j], 0 if (j - 1 < 0) else matrix[i][j - 1]]\n",
    "            chng_nbr_ix = [(1, 1), (1, 0), (0, 1)]\n",
    "            matrix[i][j] = np.max(neighbors)\n",
    "            max_nbr = np.argmax(neighbors)\n",
    "            if max_nbr == 0 and matched == 1:\n",
    "                path_matrix[i][j] = tuple((path_matrix[i-1][j-1], (i, j)))\n",
    "            else:\n",
    "                di, dj = chng_nbr_ix[max_nbr]\n",
    "                path_matrix[i][j] = path_matrix[i - di][j - dj]\n",
    "            if matrix[i][j] > overall_max:\n",
    "                max_ix = (i, j)\n",
    "                overall_max = matrix[i][j]\n",
    "    max_ix = np.argmax(matrix)\n",
    "    # print(\"The max index:\", max_ix)\n",
    "    max_row, max_col = max_ix // len(matrix[0]), max_ix % len(matrix[0])\n",
    "    # print(max_row, max_col)\n",
    "    # print(matrix[max_row][max_col])\n",
    "    # print(path_matrix[max_row][max_col])\n",
    "\n",
    "    return list(zip(*unroll_path(path_matrix[max_row][max_col])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def bar_plot_features(human_features, adv_features, feature_names, xtitle, ytitle):\n",
    "    total_width = 1.5\n",
    "    start = 0.1\n",
    "    bar_offset = 0.2\n",
    "    human_coords = [start + ((total_width + bar_offset) * i) for i in range(len(human_features))]\n",
    "    adv_coords = [start + (total_width/2) + ((total_width + bar_offset) * i) for i in range(len(human_features))]\n",
    "    plt.bar(human_coords, human_features, width=total_width/2)\n",
    "    plt.bar(adv_coords, adv_features, width=total_width/2)\n",
    "    plt.xticks([coord - (total_width/4) for coord in adv_coords], feature_names)\n",
    "    plt.xlabel(xtitle)\n",
    "    plt.ylabel(ytitle)\n",
    "    plt.legend([\"Human Use\", \"Adversarial Use\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_unmatched(vec, feat):\n",
    "    to_ret = []\n",
    "    for i in range(len(vec)):\n",
    "        if i not in feat:\n",
    "            to_add = vec[i]\n",
    "            if vec[i].startswith('Ġ'):\n",
    "                to_add = to_add[1:]\n",
    "            to_add = to_add.lower()\n",
    "            to_ret.append(to_add)\n",
    "    return to_ret\n",
    "\n",
    "human_features = set()\n",
    "adv_features = set()\n",
    "for i in range(0, len(sampler.candidate_conversations), 2):\n",
    "    human_res = sampler.candidate_conversations[i].split('\\n')[-1]\n",
    "    adv_res = sampler.candidate_conversations[i + 1].split('\\n')[-1]\n",
    "    t = scorer.get_tokenizer()\n",
    "    vec1 = t.tokenize(human_res)\n",
    "    vec2 = t.tokenize(adv_res)\n",
    "\n",
    "    feat2, feat1 = get_feature_diff(vec1, vec2)\n",
    "    hum_resp_feats = get_unmatched(vec1, feat1)\n",
    "    adv_resp_feats = get_unmatched(vec2, feat2)\n",
    "    human_features |= set(hum_resp_feats)\n",
    "    adv_features |= set(adv_resp_feats)\n",
    "    # print(\"----------------------\")\n",
    "human_features = list(human_features)\n",
    "adv_features = list(adv_features)\n",
    "adv_features.sort(key=lambda x: -vimp_adv_norm[x])\n",
    "human_features.sort(key=lambda x: -vimp_human_norm[x])\n",
    "topk = 10\n",
    "topk_hfeats = [vimp_human_norm[feat] for feat in human_features[:topk] if feat in vimp_adv_norm]\n",
    "topk_afeats = [vimp_adv_norm[feat] for feat in human_features[:topk] if feat in vimp_adv_norm]\n",
    "bar_plot_features(topk_hfeats, topk_afeats, [feat for feat in human_features[:topk] if feat in vimp_adv_norm], \"Most Important Pronouns\", \"Importance\")\n",
    "\n",
    "# print(f\"Normalized importance of top {topk} features when used in adversarial setting.\")\n",
    "# print(\"{:<11} {:<20} {:<20}\".format(\"Feature\", \"Natural Use\", \"Adversarial Use\"))\n",
    "# for feat in human_features[:topk]:\n",
    "#     if feat not in vimp_adv_norm:\n",
    "#         continue\n",
    "#     print(\"{:<11} {:<20} {:<20}\".format(feat, round(vimp_human_norm[feat], 2), round(vimp_adv_norm[feat], 2)))\n",
    "# print(adv_features[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conv_number = 0\n",
    "shap.initjs()\n",
    "for ix in [conv_number * 2, (conv_number * 2) + 1]:\n",
    "    vocab_importance = get_vocab_importance(args, dict(), shap_values[ix], conversations[ix])\n",
    "    imp_dict = {word: vocab_importance[word][0]/vocab_importance[word][1] for word in vocab_importance}\n",
    "    imp_dict = normalize_importance(imp_dict)\n",
    "    print('Original Conversation:')\n",
    "    print(sampler.candidate_conversations[ix])\n",
    "    print(\"-----------------\")\n",
    "    vocab_expl = shap.Explanation(values=[imp_dict[k] for k in imp_dict.keys()], \\\n",
    "        data=[k for k in vocab_importance.keys()])\n",
    "    shap.plots.bar(vocab_expl, show=False)\n",
    "    # shap.plots.text(vocab_expl)\n",
    "    plt.figure()\n",
    "    # plt.savefig(\"interpretability_example.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "imp_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for conv in conversations:\n",
    "count = 0\n",
    "for i in range(len(sampler.candidate_conversations)//2):\n",
    "    conv1 = sampler.candidate_conversations[i * 2]\n",
    "    conv2 = sampler.candidate_conversations[(i * 2) + 1]\n",
    "    # print(conv1)\n",
    "    # print(\"----------\")\n",
    "    # print(conv2.strip())\n",
    "    # print(\"##########\")\n",
    "    last_utterance1 = conv1.split('\\n')[-1]\n",
    "    last_utterance2 = conv2.split('\\n')[-1]\n",
    "    tokenizer = scorer.get_tokenizer()\n",
    "    count += (len(tokenizer.tokenize(last_utterance1)) < len(tokenizer.tokenize(last_utterance2)))\n",
    "print(f\"Proportion of increased lengths: {count/len(sampler.candidate_conversations)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "property_under_consideration = 'understand'\n",
    "np.mean(shap_values[:, :, property_under_consideration].mean(0).values), \\\n",
    "    np.std(shap_values[:, :, property_under_consideration].mean(0).values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pickle.dump(shap_values, open('kernelshap_values.pkl', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ['interesting', 'engaging', 'specific', 'relevant', 'correct', \\\n",
    "#                 'semantically appropriate', 'understandable', 'fluent', 'coherent', 'error recovery', \\\n",
    "#                 'consistent', 'diverse', 'depth', 'likeable', 'understand', 'flexible', 'informative', 'inquisitive']\n",
    "# shap_values[:, :, 0].mean(0).shape\n",
    "# shap.initjs()\n",
    "# shap.plots.bar(shap_values[:, :, 0].mean(0), max_display=10)\n",
    "# shap_values[0, :250]\n",
    "# print(next(sampler.get_next()))\n",
    "scores = dict()\n",
    "conv_tokens = scorer._tokenizer.tokenize(scorer.format_conversations([next(sampler.get_next())])[0])\n",
    "for i, token in enumerate(conv_tokens):\n",
    "    scores.setdefault(token, (0, 0))\n",
    "    scores[token] = (scores[token][0] + shap_values[0][i], scores[token][1] + 1)\n",
    "scores\n",
    "expl_obj = shap.Explanation([scores[k][0]/scores[k][1] for k in scores.keys()], data=list(scores.keys()))\n",
    "shap.plots.bar(expl_obj)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c9381a82c3292c6489ed672d01ee1fb27292e41fb2b5d92e4e1822959879bf7"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('internship-env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}